---
title: "p8105_hw3_jg5038"
author: "Julia Gray"
date: "2025-10-02"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(patchwork)
library(p8105.datasets)

#these options were messing up my fig.height 
#knitr::opts_chunk$set(
#  fig.width = 6,
#  fig.asp = .6
#)

knitr::opts_chunk$set(
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


# Problem 1

```{r message=FALSE}
data("instacart")

instacart_df = instacart |> 
  janitor::clean_names()
```

order_dow is given as an integer but to make it human readable let's change it to a string. We don't know which day is which but in order to find out we can see the distribution by day and assume that the most orders happen on Monday (since on Saturday and Sunday people are more likely to go grocery shopping themselves).

```{r}
instacart_df |> 
  group_by(order_dow) |> 
  summarize(n = n_distinct(order_id))

#since 0 had the highest number of orders let's make that Monday
instacart_df = instacart_df |> 
  mutate(order_dow_string = 
         case_match(
           order_dow, 
           0 ~ "Mon", 
           1 ~ "Tues",
           2 ~ "Weds",
           3 ~ "Thurs", 
           4 ~ "Fri",
           5 ~ "Sat",
           6 ~ "Sun"
         ))
```

This dataset contains order information from instacart. It is organized by order and contains information about the products ordered, as well as information about the customer (i.e. how long it's been since they last ordered). It contains `r dim(instacart)[1]` observations and includes columns for `r colnames(instacart)`. 

For example, the first order in the dataset was made by `r unique(select(filter(instacart, order_id==1), user_id))` at `r unique(select(filter(instacart, order_id==1), order_hour_of_day))` o clock and we can see they ordered `r toString(select(filter(instacart, order_id==1), product_name))` (of which `r toString(select(filter(instacart, order_id==1, reordered==1), product_name))` were reordered).

There are `r dim(unique(select(instacart, aisle)))[1]` unique aisles. Here is a table of the aisles most ordered from (just showing the top 10):

```{r}
instacart_aisle_sum_df = instacart_df |> 
  group_by(aisle) |> 
  summarize(
    n = n()
  ) |> 
  arrange(desc(n)) 

instacart_aisle_sum_df |> 
  head(10) |> 
  knitr::kable()
```

Now let's plot aisles with more than 10,000 items ordered from them:

```{r fig.width=6, fig.height=6}
instacart_aisle_sum_df |> 
  filter(n >= 10000) |> 
  ggplot(aes(x=reorder(aisle, n), y=n)) +
  geom_bar(stat="identity") +
  labs(
        title = "Aisles with Most Orders from Instacart",
        y = "Items Ordered",
        x = "Aisle",
        caption = "Aisles with >=10,000 items ordered"
      ) +
  coord_flip()
```

In order to find the most popular items for each aisle let's use group by:

```{r}
instacart_aisle_item_df = instacart_df |> 
  group_by(aisle) |> 
  count(product_name) |> 
  mutate(
    item_aisle_rank = min_rank(desc(n))
    ) |> 
  filter(aisle %in% c('baking ingredients', 'dog food care', 'packaged vegetables fruits'), item_aisle_rank <= 3) |> 
  arrange(aisle, item_aisle_rank)

knitr::kable(instacart_aisle_item_df)
```

To make a table showing the mean hour per day at which items are ordered for each day of the week we will group by day and calculate the mean:

```{r}
instacart_df |> 
  filter(product_name %in% c('Pink Lady Apples', 'Coffee Ice Cream')) |> 
  group_by (product_name, order_dow_string) |> 
  summarize(
    mean_order_hour_of_day = round(mean(order_hour_of_day, na.rm = TRUE), digits=2)
  ) |> 
  pivot_wider(
    names_from = order_dow_string,
    values_from = mean_order_hour_of_day
  ) |> 
  select('product_name', 'Mon', 'Tues', 'Weds', 'Thurs', 'Fri', 'Sat', 'Sun') |> 
  knitr::kable()
```

# Problem 2

Import and tidy data
```{r message=FALSE}
zip_df = read_csv(file="./data/zillow_data/Zip Codes.csv") |> 
  janitor::clean_names()

rent_df = read_csv(file="./data/zillow_data/Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv") |> 
  janitor::clean_names() |> 
  #update column names to match zip_df
  rename(
    zip_code = region_name,
    county = county_name) |> 
  mutate(
    county = str_replace(county, " County", ""),
    borough = 
      case_match(
        county, 
        "Bronx" ~ "Bronx", 
        "Kings" ~ "Brooklyn",
        "New York" ~ "Manhattan",
        "Queens" ~ "Queens", 
        "Richmond" ~ "Staten Island") 
  ) |> 
  pivot_longer(
    cols=x2015_01_31:x2024_08_31,
    names_to = "date",
    values_to="observed_rent_index"
  ) |> 
  mutate(
    #let's use the lubridate package
    date = ymd(str_replace(date, "x", "")),
    year = year(date),
    month = month(date)
  )

rent_zip_df = left_join(rent_df, zip_df, by = c("county", "zip_code")) |> 
  relocate("zip_code", "neighborhood",  "date", "observed_rent_index")
```

In order to find how many zipcodes are observed for all 116 months and how many observed less than 10 times, we will use summarize:

```{r}
rent_zip_na = rent_zip_df |> 
  group_by(zip_code) |> 
  summarize(
    n_observations = sum(!is.na(observed_rent_index))
  )
```

There are `r dim(filter(rent_zip_na, n_observations==116))[1]` zipcodes with 116 observations and `r dim(filter(rent_zip_na, n_observations<10))[1]` with less than 10. The zipcodes with 116 observations are popular residential districts and the ones with less than 10 are mixed between residential and commercial real estate (for example 10044 is Roosevelt Island). 

```{r}
rent_zip_df |> 
  group_by(borough, year) |> 
  summarize(
    average_rent_index = round(mean(observed_rent_index, na.rm = TRUE), digits=2)
  ) |> 
  pivot_wider(
    names_from = year,
    values_from = average_rent_index
  ) |> 
  knitr::kable()
```

Unsurprisingly, Manhattan has the highest average rent index. There is no data for Staten Island prior to 2020. We can see the decrease caused by COVID across the other boroughs.

```{r message = FALSE}
year_rent_graph = rent_zip_df |> 
  group_by(zip_code, borough, year) |> 
  summarize(
    average_rent_index = round(mean(observed_rent_index, na.rm = TRUE), digits=2)
  ) |> 
  ggplot(aes(x = year, y=average_rent_index, color=borough)) + 
  geom_point(alpha = 0.5, show.legend=FALSE) +
  geom_smooth(se = FALSE, show.legend=FALSE) + 
  labs(
        title = "Average Yearly Rent Index by Borough from 2015 to 2024",
        x = "Year",
        y = "Average Rent Index"
      ) +
  scale_x_continuous(breaks=seq(2015, 2024, 5)) +
  facet_grid(. ~ borough)

year_rent_graph
```

We can see Manhattan, in addition to having the highest average rent index, has the highest variance among all boroughs. 

Let's see some boxplots for Manhattan for fun:

```{r}
rent_zip_df |> 
  filter(borough == 'Manhattan') |> 
  group_by(zip_code, year) |> 
  ggplot(aes(x = year, y=observed_rent_index, group = year)) +
  labs(
        title = "Average Yearly Rent Index in Manhattan from 2015 to 2024",
        x = "Year",
        y = "Average Rent Index"
      ) +
  scale_x_continuous(breaks=seq(2015, 2024, 1)) +
  geom_boxplot()
```

Since each row is already 1 zipcode per month there is nothing to average, it will just be the rent index in the row. We can confirm this because when we calculate the mean it is the same as if we just filtered and grouped by (since the n is 1).

```{r}
month_rent_graph = rent_zip_df |>
  filter(year == 2023) |> 
  group_by(zip_code, borough, month) |> 
  summarize(
    average_rent_index = round(mean(observed_rent_index, na.rm = TRUE), digits=2)
  ) |> 
  ggplot(aes(x = month, y=average_rent_index, color=borough, group=month)) +
  #geom_point() +
  labs(
        title = "Monthly Average Rent Index by Borough during 2023",
        x = "Month",
        y = "Average Rent Index"
      ) +
  scale_x_continuous(breaks=seq(1, 12, 3)) +
  geom_boxplot(show.legend=FALSE) +
  facet_grid(. ~ borough)

month_rent_graph
```

Again we can see the higher average and higher variance in Manhattan and relatively low variance in Staten Island. There does not seem to be much difference across the months of year although there is a faint bump in June-August across all boroughs (especially in Manhattan), which likely corresponds to students coming to New York City for a summer internship. 


Combine the two graphs:

```{r fig.height=8}
year_rent_graph / month_rent_graph + plot_layout(guide="collect")
ggsave('plots/zillow_rent_index_by_borough.pdf')
```

# Problem 3

Load, tidy, merge the dataset

```{r}
accel_df = read.csv('./data/nhanes/nhanes_accel.csv') |> 
  janitor::clean_names() |> 
  #use colnames(accel_df)[ncol(accel_df)] to get last colname
  pivot_longer(
    cols=min1:min1440,
    names_to = "min",
    #names_prefix("min"),
    values_to="MIMS"
  ) |> 
  mutate (
    min = as.integer(str_remove(min, "min"))
  )

covar_df = read.csv('./data/nhanes/nhanes_covar.csv', skip = 4) |> 
  #let's exlcude participants under 21, or missing demographic data before we merge
  janitor::clean_names() |> 
  filter(age >= 21, !is.na(sex), !is.na(bmi), !is.na(education)) |> 
  mutate(
    sex = case_match(
      sex, 
      1 ~ "male",
      2 ~ "female"
    ),
    education_factor = factor(
      education, 
      levels = c(1, 2, 3),
      labels = c("less than highschool", "highschool", "more than highschool")
    )
  )

nhanes_df = left_join(covar_df, accel_df, by="seqn")
```

Let's produce a reader-friendly table for the number of men and women in each education category:

```{r}
nhanes_df |> 
  group_by(sex, education_factor) |> 
  summarize(
    n = n_distinct(seqn)
  ) |> 
  pivot_wider(
    names_from = education_factor,
    values_from = n
  ) |> 
  knitr::kable()
```

Let's create vizualization of age distribution for men and women in each age category:

```{r}
nhanes_df |> 
  select(seqn, sex, age, education_factor) |> 
  distinct() |> 
  ggplot(aes(x=age, color=sex)) +
  geom_histogram(aes(fill=sex), binwidth=5, show.legend=FALSE) +
  labs(
        title = "Age Distribution by Education Level",
        x = "Age",
        y = "Count"
      ) +
  facet_grid(sex ~ education_factor)

#we can double check this is correct by using the original demographics data
#covar_df |> 
#  ggplot(aes(x=age, color=sex)) +
#  geom_histogram(aes(fill=sex), binwidth=5) +
#  facet_grid(sex ~ education)
```

We have the most participants in the 'more than highschool' education category and this is especially pronounced for women under 40.

First, let's create a plot of total activity against age:

Let's make sure we're getting all participants:
```{r}
length(unique(pull(nhanes_df, seqn)))
```

```{r}
nhanes_df |> 
  group_by(seqn, sex, age, education_factor) |> 
  summarize(
    total_activity = sum(MIMS)
  ) |> 
  ggplot(aes(x=age, y=total_activity, color=sex)) +
  geom_point(alpha = 0.5) +
  geom_smooth(se = FALSE) +
  labs(
        title = "Total Activity by Age",
        x = "Age",
        y = "Total Activity (mins)"
      ) +
  facet_grid(education_factor ~ .)
```

We can see that generally activity goes down with age and this was more pronounced in the participants with less than high school education. There was no significant different between men and women in any of the education groups 

```{r}
#Test with a single person
#nhanes_df |> 
#  filter(seqn==62161) |> 
#  ggplot(aes(x=min, y=MIMS)) +
#  geom_point()

nhanes_df |> 
  group_by(min, sex, education_factor) |> 
  summarize(
    mean_mims = mean(MIMS, na.rm = TRUE),
    median_mims = median(MIMS, na.rm = TRUE)
  ) |> 
  ggplot(aes(x=min, y=median_mims, color=sex)) +
  geom_point(alpha = 0.5) +
  geom_smooth(se=FALSE) +
  labs(
        title = "Median Activity throughout the Course of a Day",
        x = "Time (mins)",
        y = "Median MIMS"
      ) +
  scale_x_continuous(
    name = "Time (hour)", #override mins labels
    labels = function(x) x / 60,
    breaks = seq(0, 1450, 60)
  ) +
  facet_grid(education_factor ~ .)
```

We can see that there is a general arc of activity throughout the day across all groups, which makes sense. In the education groups with highschool or more than highschool education we can see that females were slightly more active in the morning and middle of the day. 















